install.packages("far")
library(far)
# Function to calculate BIC
calculate_bic = function(Y, A, S)
# Y: connectivity data of dimension N x K, N is number of replicates/samples, K is number of edges.
# A: mixing matrix of dimension N x q, where q is the number of latent sources.
# S: latent sources of dimension q by K.
{
N = dim(Y)[1]
K = dim(Y)[2]
# define a norm function
norm_vec = function(x) sum(x^2)
# calculate the residual standard deviation
sigma = sqrt(1/(N*K)*sum(apply(Y - A%*%S, MARGIN = 1, norm_vec)))
# calculate loglikelihood
loglike = 0
for (j in 1:N){
mean = as.vector(A[j,] %*% S)
loglike = loglike - 2 * sum(log(dnorm(Y[j,], mean, sigma)))
}
# calculate logN*sum(||S||0)
L0 = log(N)*sum(abs(S)>1e-1)
# calculate bic
BIC = loglike + L0
return(BIC)
}
# Use BIC method to select the hyper-parameters of phi and rho.
LOCUS_BIC_selection <- function(Y, q, V, MaxIteration=50, penalty="SCAD", phi_grid_search=seq(0.2, 1, 0.2),
rho_grid_search=c(0.95), espli1=0.001, espli2=0.001, save_LOCUS_output=T,
preprocess = T, demean = T)
# Y: connectivity data of dimension N x K, N is number of subjects, K is number of edges.
# q: Number of subnetworks to extract.
# V: Number of nodes in network.
# MaxIteration = 100: number of maximum iterations.
# espli1=0.001, espli2=0.001: toleration for convergence on S and A.
# penalty = "SCAD": sparsity penalty, this can be NULL, SCAD, L1 or Hardthreshold.
# phi_grid_search: grid search candidates of tuning parameter for penalty
# rho_grid_search: grid search candidates of tuning parameter for selecting number of ranks in each subnetwork's decomposition.
{
# demean or preprocess Y to speed up
if(demean){
Y = sweep(Y,2,apply(Y,2,mean),"-")
}
if(preprocess){
Y = Locus_preprocess(Y, q)
}
# run grid search
LOCUS_results = list()
bic_tab = matrix(0, ncol = 3, nrow=length(rho_grid_search) * length(phi_grid_search))
k = 1
for(rho in rho_grid_search)
{
for(phi in phi_grid_search)
{
print(paste0('Running LOCUS for rho=', rho, ', phi=', phi, '...'))
LOCUS_output = LOCUS(Y, q=q, V=V, MaxIteration=MaxIteration, penalty=penalty, phi=phi, approximation=T,
preprocess=F, espli1=espli1, espli2=espli1, rho=rho, silent=T, demean=F)
bic_value = calculate_bic(Y, LOCUS_output$A, LOCUS_output$S)
bic_tab[k,] = c(rho, phi, bic_value)
if(save_LOCUS_output)
{
LOCUS_results[[k]] = list(LOCUS=LOCUS_output, rho=rho, phi=phi)
}
k = k+1
}
}
if(save_LOCUS_output)
{
return(list(bic_tab = bic_tab, LOCUS_results = LOCUS_results))
}
return(list(bic_tab = bic_tab))
}
S_ini[l,] = S_ini[l,] * scaleL
Locus_initial <- function(Y,q,V,rho=0.95,R = NULL,maxIter = 100)
{
ICcorr = icaimax(t(Y),nc = q,center=F,maxit=maxIter)
S_ini = matrix(0,ncol=dim(ICcorr$S)[1],nrow=q)
theta_ini = list()
for(i in 1:q)
{
Sl = Ltrinv( ICcorr$S[,i],V,F)
Sl = Sl + diag( rep(mean(ICcorr$S[,i]),V ))
eigenSl = eigen(Sl)
orderEigen = order(abs(eigenSl$values),decreasing = T)
if(is.null(R))
{
Rl = 2
while( TRUE )
{
eigenset = orderEigen[1:Rl]
imgeRL = eigenSl$vectors[,eigenset]%*% diag(eigenSl$values[eigenset])%*% t(eigenSl$vectors[,eigenset])
# image( imgeRL )
if(cor(Ltrans(imgeRL,F),ICcorr$S[,i]) > rho) break
Rl = Rl + 1
}
}else
{
Rl = R[i]; eigenset = orderEigen[1:Rl]
}
theta_ini[[i]] = list()
theta_ini[[i]]$lam_l = eigenSl$values[ eigenset ]
if( theta_ini[[i]]$lam_l[1]<0 ){theta_ini[[i]]$lam_l = -1*theta_ini[[i]]$lam_l}
theta_ini[[i]]$X_l = matrix(0,ncol = V, nrow = Rl)
for(j in 1:Rl)
{
theta_ini[[i]]$X_l[j,] = eigenSl$vectors[,eigenset[j]]
}
S_ini[i,] = Ltrans( t(theta_ini[[i]]$X_l)%*%diag(theta_ini[[i]]$lam_l)%*%theta_ini[[i]]$X_l,F)
}
# compute initial value of A
A_ini = Y%*%t(S_ini)%*%solve(S_ini%*%t(S_ini))
# scale up
for(l in 1:q){
# unit norm each column of A
# scaleL = sqrt(sum(A_ini[,l]^2))
scaleL = sd(A_ini[,l])
A_ini[,l] = A_ini[,l] / scaleL
S_ini[l,] = S_ini[l,] * scaleL
# scale X_l correspondingly
theta_ini[[l]]$X_l = theta_ini[[l]]$X_l * sqrt(scaleL)
}
# since after preprocessing, A_tilde is orthogonal
# compute A_tilde transpose/inverse (g-inverse of A-tilde)
# if X has full column rank, (X'X)^(-1)X' is its g-inverse
# why use g-inverse here: since A_ini is N*q
# afterwards, in the update process, we actually can just use A_tilde transpose(after scale), or g-inverse
M_ini =  solve(t(A_ini)%*%A_ini)%*%t(A_ini) # g-inverse of A
for(l in 1:q){
theta_ini[[l]]$M_l = M_ini[l,]
}
return(list(A=A_ini,theta = theta_ini,S = S_ini))
}
Locus_preprocess <-function(Ynew,q)
{
N = dim(Ynew)[1]
eigenA = eigen( Ynew %*% t(Ynew), T )
whitenmat = diag( (eigenA$values[1:q] - mean(eigenA$values[(q+1):N]))^(-0.5)  ) %*% t(eigenA$vectors[,1:q])
Ynew = whitenmat%*%Ynew # the whitened A matrix
Ynew = Ynew/sd(Ynew)*5
return(Ynew)
}
Locus_update_approx <- function(Y,A,theta,penalt = NULL,lambda_ch = 0.5,gamma = 3,imput_method = "Previous",silent = F)
{
# An extremely efficient approximation method with potentially higher performance.
if(is.null(penalt))
{
if(!silent)
print(paste("Locus without penalty."))
}else{
if(!silent)
print(paste("Locus with", penalt,"penalty."))
}
theta_new = list()
K = dim(Y)[2]
V = (sqrt(1+8*K)+1)/2
N = dim(Y)[1]
q = length(theta)
R = vector()
for(curr_ic in 1:q)
{
theta_ic = theta[[curr_ic]]
R[curr_ic] = dim(theta_ic$X_l)[1]
S_lold = t(theta_ic$M_l%*%Y)
if(is.null(penalt))
{
S_new_0 = S_lold
}else if(penalt == "SCAD")
{
if(gamma<=2){print("Gamma needs to be > 2!");gamma = 2.01}
S_new_0= SCAD_func(S_lold,lambda_ch = lambda_ch  ,gamma = gamma)
S_new_0 = S_new_0 /sd(S_new_0)*sd(S_lold)
}else if(penalt == "Hardthreshold")
{
S_new_0 = S_lold*(abs(S_lold)>=lambda_ch)
}else if(penalt == "L1")
{
S_new_0 = sign(S_lold)*(abs(S_lold)-lambda_ch)*(abs(S_lold)>=lambda_ch)
S_new_0 = S_new_0 /sd(S_new_0)*sd(S_lold)
}else
{
print("No Penalty available!")
stop()
}
if(imput_method == "Previous")
{
Sl = Ltrinv(S_new_0,V,F) + diag(diag(t( theta_ic$X_l)%*%diag(theta_ic$lam_l)%*%theta_ic$X_l ))
}else if(imput_method == "Average")
{ Sl = Ltrinv(S_new_0,V,F) + diag( rep(mean(S_new_0),V ))
}else{print("No Imputation available!")
stop()}
eigenSl = eigen(Sl)
orderEigen = order(abs(eigenSl$values),decreasing = T)
Rl = R[curr_ic]
eigenset = orderEigen[1:Rl]
theta_new[[curr_ic]] = list()
theta_new[[curr_ic]]$lam_l = eigenSl$values[ eigenset ]
if( theta_new[[curr_ic]]$lam_l[1]<0 )
{
theta_new[[curr_ic]]$lam_l = -1*theta_new[[curr_ic]]$lam_l
}
for(j in 1:Rl)
{
theta_ic$X_l[j,]= eigenSl$vectors[,eigenset[j]]
}
theta_new[[curr_ic]]$X_l = theta_ic$X_l
}
# Update A
l = 1; Snew = array(dim=c(q,K))
while(l <= q)
{
Snew[l,] = Ltrans(t(theta_new[[l]]$X_l)%*% diag(theta_new[[l]]$lam_l) %*%theta_new[[l]]$X_l,F)  # K x M
l = l+1
}
## estimate A
Anew = Y%*%t(Snew) %*% solve(Snew%*%t(Snew))
for(i in 1:q)
{
ai = sd(Anew[,i])
theta_new[[i]]$lam_l = theta_new[[i]]$lam_l * ai
Anew[,i] =Anew[,i] / ai
Snew[i,] = Snew[i,] * ai
}
# Save m_l, X_l into theta2_new:
Mnew =  solve(t(Anew)%*%Anew)%*%t(Anew)
for(l in 1:q)
{
theta_new[[l]]$M_l = Mnew[l,]
}
return(list(A = Anew,S=Snew,theta = theta_new))
}
Locus_update <- function(Y,A,theta,penalt = NULL,lambda_ch = 0.5,gamma = 3,silent = F)
{
# Update latent channels X's with flat prior
# A denotes mixing matrix
# theta contains IC-specific parameters and channels
if(is.null(penalt))
{
if(!silent)
print(paste("Locus without penalty."))
}else{
if(!silent)
print(paste("Locus with", penalt,"penalty."))
}
theta_new = list()
#require(MASS)
K = dim(Y)[2]         # Number of edges
V = (sqrt(1+8*K)+1)/2
N = dim(Y)[1]
q = length(theta)
rmat = matrix(rep(1:V,V),ncol=V)
cmat = t(rmat)
Lcoorsym = matrix(0,ncol=2,nrow=V*(V-1)/2)
Lcoorsym[,1] = Ltrans(rmat,F)
Lcoorsym[,2] = Ltrans(cmat,F)
## For each IC, conditioned on others, estimate latent channels X:
newS = array(dim=c(q,K))
for(curr_ic in 1:q)
{
theta_new[[curr_ic]] = list()
# Update X:
theta_ic = theta[[curr_ic]]
Dinverse = 1/theta[[curr_ic]]$lam_l           # D^(-1): of length R
theta[[curr_ic]]$X_l[is.na(theta[[curr_ic]]$X_l)] = 0
Yic = t(theta[[curr_ic]]$M_l%*%Y)             # p x 1
v = 1
while(v <= V)
{
Hlv = t(theta[[curr_ic]]$X_l[,-v])                    # X(-v) which is V-1 x R
yvpen = Yic[which(Lcoorsym[,1] == v | Lcoorsym[, 2] == v),]               # Y
Sigmalv = psdmat_inverse( t(Hlv)%*%Hlv )
if(is.null(penalt))
{
beta = yvpen
}else if(penalt == "SCAD")
{
if(gamma<=2){print("Gamma needs to be > 2!");gamma = 2.01}
beta= SCAD_func(yvpen,lambda_ch = lambda_ch  ,gamma = gamma)
}else if(penalt == "L1")
{
beta = sign(yvpen)*(abs(yvpen)-lambda_ch)*(abs(yvpen)>=lambda_ch)
}else if(penalt == "Hardthreshold")
{
beta = yvpen*(abs(yvpen)>=lambda_ch)
}else
{
print("No Penalty available!")
stop()
}
if(sd(beta) == 0)
{
theta[[curr_ic]]$X_l[,v] = 0
}else
{
theta[[curr_ic]]$X_l[,v] = (Dinverse*(Sigmalv%*%t(Hlv)%*%beta)) /sd(beta)*sd(yvpen) # Rx1
}
v = v+1
}
theta_new[[curr_ic]]$X_l = theta[[curr_ic]]$X_l
# check the rank
#if(theta[[curr_ic]]$X_l){}
# Update D:
Xstarstack = t(apply(theta[[curr_ic]]$X_l,1,function(x){ x = matrix(x,ncol=1);return(Ltrans(x%*%t(x),F)) }))
if(is.null(penalt))
{
beta = Yic
}else if(penalt == "SCAD")
{
if(gamma<=2){print("Gamma needs to be > 2!");gamma = 2.01}
beta= SCAD_func(Yic,lambda_ch = lambda_ch  ,gamma = gamma)
}else if(penalt == "L1")
{
beta = sign(Yic)*(abs(Yic)-lambda_ch)*(abs(Yic)>=lambda_ch)
}else if(penalt == "Hardthreshold")
{
beta = Yic*(abs(Yic)>=lambda_ch)
}
theta_new[[curr_ic]]$lam_l = as.numeric(solve(Xstarstack%*%t(Xstarstack))%*%Xstarstack%*%beta /sd(beta)*sd(Yic)) # Rx1
newS[curr_ic,] = Ltrans(t(theta_new[[curr_ic]]$X_l)%*%
diag(theta_new[[curr_ic]]$lam_l) %*%
theta_new[[curr_ic]]$X_l,F)
}
# Update A
newA = Y%*%t(newS) %*% solve(newS%*%t(newS))
# Orthnologize A
# newA = newA %*% real( solve( t(newA)%*%newA )^(1/2)); # This could lead to numerical problems.
for(i in 1:q)
{
ai = sd(newA[,i])
theta_new[[i]]$lam_l = theta_new[[i]]$lam_l * ai
newA[,i] = newA[,i] / ai
newS[i,] = newS[i,] * ai
}
# Save m_l, X_l into theta_new:
Mnew =  solve(t(newA)%*%newA)%*%t(newA) # Generalized inverse of A
for(l in 1:q)
{
theta_new[[l]]$M_l = Mnew[l,]
}
return(list(A=newA,theta=theta_new,S=newS))
}
LOCUS <- function(Y, q, V, MaxIteration=100, penalty="SCAD", phi = 0.9,
approximation=T, preprocess=T,
espli1=0.001, espli2=0.001, rho=0.95, silent=F, demean=T)
# Y: connectivity data of dimension N x K, N is number of subjects, K is number of edges.
# q: Number of subnetworks to extract.
# V: Number of nodes in network.
# MaxIteration = 100: number of maximum iterations.
# espli1=0.001, espli2=0.001: toleration for convergence on S and A.
# penalty = "SCAD": sparsity penalty, this can be NULL, SCAD, L1 or Hardthreshold.
# phi = 0.02: tuning parameter for penalty
# rho = 0.95: tuning parameter for selecting number of ranks in each subnetwork's decomposition.
# silent: whether to print intermediate results.
# preprocess: whether to preprocess the data Y (dont change if you are not sure)
# approximation = T: whether to use approximated algorithm based on SVD (dont change if you are not sure).
{
if(demean)
{
Y = sweep(Y,2,apply(Y,2,mean),"-")
}
if(preprocess)
{
Yraw = Y
Y = Locus_preprocess(Y,q)
}
K = dim(Y)[2]          # Number of edges
if(V != (sqrt(1+8*K)+1)/2)
{
print("V is not correctly specified! Please double check the dimension of your input data.")
stop()
}
N = dim(Y)[1]          # Number of subjects (ICs)
# Initial Estimation
theta_ini = Locus_initial(Y,q,V,rho=rho)
A = theta_ini$A; S = theta_ini$S
theta = theta_ini$theta
# Update Parameters
Iter = 1
while(Iter <= MaxIteration)
{
if(approximation)
{
theta_new = Locus_update_approx(Y,A,theta,penalt= penalty,lambda_ch = phi, gamma = 2.1,imput_method = "Previous",silent = silent)
}else
{
theta_new = Locus_update(Y,A,theta,penalt= penalty,lambda_ch = phi, gamma = 2.1,silent = silent)
}
# orthogonize A here
if(preprocess){
A_new = orthonormalization(theta_new$A)
}else{
A_new = theta_new$A
}
S_new = theta_new$S; theta_new  = theta_new$theta
errS = norm(as.matrix(S_new-S))/norm(as.matrix(S))
errA = norm(as.matrix(A_new-A))/norm(as.matrix(A))
if(sum(is.na(c(errS,errA)))>0){return(list(Conver=F,A=A,S=S,theta=theta))}
if(!silent)
{
print(paste("Iter ",Iter,"; Percentage change on S: " , round(errS,3),"; Percentage change on A: ",round(errA,3),".",sep=""))
}
A = A_new; S= S_new; theta = theta_new
# print(performance_ASq3(S,Struth,Atrue,Atrue)) # this is only for simulation test
if(errA < espli1 & errS < espli2)
{
if(!silent){print("Converaged!")}
if(preprocess){
A = Yraw %*% t(S) %*% solve(S%*%t(S))
}else{
# demeaned Y or unpreprocessed+undemeaned Y
A = Y %*% t(S) %*% solve(S%*%t(S))
}
return(list(Conver = T,A=A,S=S,theta=theta))
}
Iter = Iter + 1
}
if(!silent){print("Failed to converge!")}
if(preprocess){
A = Yraw %*% t(S) %*% solve(S%*%t(S))
}else{
# demeaned Y or unpreprocessed+undemeaned Y
A = Y %*% t(S) %*% solve(S%*%t(S))
}
return(list(Conver=F, A=A, S=S, theta=theta))
}
Ltrans<-function(X,d=T){ X[upper.tri(X,d)]  }
Ltrinv<-function(x,V,d=T){ Y = matrix(0,ncol = V,nrow = V);
Y[upper.tri(Y,d)]=x;return(Y + t(Y) -d*diag(diag(Y)))  }
psdmat_inverse<-function(mat)
{
# Originally defined for PSD matrices
p = dim(mat)[1]
eigendecomp = eigen(mat)
#if(-min(eigendecomp$values)> 0.0001){print("Error: Matrix has negative eigenvalue!");stop()}
if( min(eigendecomp$values)<=0.0001 )
{
print("Matrix has nearly zero eigenvalue, perturbation is added. ")
print(round(eigendecomp$values,3))
perturb <- max(max(eigendecomp$values) - p * min(eigendecomp$values), 0)/(p - 1)
}else
{
perturb = 0
}
# mat = mat + diag(p) * perturb
# eigendecomp = eigen(mat)
return( (eigendecomp$vectors)%*%diag(1/(perturb+eigendecomp$values))%*%t(eigendecomp$vectors) )
}
SCAD_func<-function(yvpen,lambda_ch=0.01,gamma=3)
{
if(gamma<=2){gamma = 2.01;print("Gamma needs > 2!!!")}
ynew = sign(yvpen)*(abs(yvpen)-lambda_ch)*(abs(yvpen)>=lambda_ch)*(abs(yvpen)<=2*lambda_ch)+
yvpen*(abs(yvpen) > gamma*lambda_ch) +
((gamma-1)*yvpen-sign(yvpen)*gamma*lambda_ch)/(gamma-2)*(abs(yvpen)<=gamma*lambda_ch)*(abs(yvpen)>2*lambda_ch)
if(sd(ynew) < 0.0000001){print("Parmeters are not correctly specified!");return(ynew)}
return(ynew)#/sd(ynew)*sd(yvpen))
}
## Simulated the data to use.
V = 50
S1 = S2 = S3 = matrix(0,ncol = V,nrow = V)
S1[5:20,5:20] = 4;S1[23:37,23:37] = 3;S1[40:48,40:48] = 3
S2[15:20,] = -3;S2[,15:20] = -3
S3[15:25,36:45] = 3; S3[36:45,15:25] = 3
Struth = rbind(Ltrans(S1,FALSE) , Ltrans(S2,FALSE), Ltrans(S3,FALSE))
set.seed(100)
Atruth = matrix(rnorm(100*3),nrow=100,ncol=3)
Residual = matrix(rnorm(100*dim(Struth)[2]),nrow=100)
Yraw = Atruth\%*\%Struth + Residual
Yraw = Atruth%*%Struth + Residual
##### Run Locus on the data #####
Locus_bic_result = LOCUS_BIC_selection(Yraw,3,V)
library(ica)
library(far)
##### Run Locus on the data #####
Locus_bic_result = LOCUS_BIC_selection(Yraw,3,V)
print(Locus_bic_result$bic_tab)
idx = which.min(Locus_bic_result$bic_tab[,3])
par(mfrow=c(2,3))
for(i in 1:3){image(Ltrinv(Struth[i,], V, FALSE))}
for(i in 1:3){image(Ltrinv(Locus_bic_result$LOCUS_results[[idx]]$LOCUS$S[i,],
V, FALSE))}
start.time = Sys.time()
## Simulated the data to use.
V = 50
S1 = S2 = S3 = matrix(0,ncol = V,nrow = V)
S1[5:20,5:20] = 4;S1[23:37,23:37] = 3;S1[40:48,40:48] = 3
S2[15:20,] = -3;S2[,15:20] = -3
S3[15:25,36:45] = 3; S3[36:45,15:25] = 3
Struth = rbind(Ltrans(S1,FALSE) , Ltrans(S2,FALSE), Ltrans(S3,FALSE))
set.seed(100)
Atruth = matrix(rnorm(100*3),nrow=100,ncol=3)
Residual = matrix(rnorm(100*dim(Struth)[2]),nrow=100)
Yraw = Atruth%*%Struth + Residual
##### Run Locus on the data #####
Locus_bic_result = LOCUS_BIC_selection(Yraw,3,V)
print(Locus_bic_result$bic_tab)
idx = which.min(Locus_bic_result$bic_tab[,3])
par(mfrow=c(2,3))
for(i in 1:3){image(Ltrinv(Struth[i,], V, FALSE))}
for(i in 1:3){image(Ltrinv(Locus_bic_result$LOCUS_results[[idx]]$LOCUS$S[i,],
V, FALSE))}
end.time = Sys.time()
start.time - end.time
